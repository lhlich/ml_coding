{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paper: [Contrastive Decoding: Open-ended Text Generation as Optimization](https://arxiv.org/abs/2210.15097)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "amateur_lm = transformers.AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
    "expert_lm = transformers.AutoModelForCausalLM.from_pretrained('gpt2-medium').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\beati\\OneDrive\\Documents\\github_repos\\ml_coding\\myenv\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built-in generate for prompt: Barack Obama was born in Honolulu, Hawaii. He was born in\n",
      "    responses: Barack Obama was born in Honolulu, Hawaii. He was born in Chicago, Illinois. He was born in New York City, New York. He was born in Arlington, Virginia. He was born in Washington, Washington. He was born in St. John's, Newfoundland. He was born in New York City, New York. He was born in Chicago, Illinois. He was born in Dallas, Texas. He was born in New York City, New York. He was born in Chicago,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Barack Obama was born in Honolulu, Hawaii. He was born in\"\n",
    "\n",
    "def generate_builtin(model, prompt, max_len=100, temperature = 0.8):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        **model_inputs,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_length=max_len,\n",
    "    )\n",
    "    return tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "print(\n",
    "    f\"\"\"built-in generate for prompt: {prompt}\n",
    "    responses: {generate_builtin(expert_lm, prompt)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[10374,   441,  2486,   373,  4642,   287, 43296,    11, 13708,    13,\n",
      "           679,   373,  4642,   287]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "My generate for prompt: Barack Obama was born in Honolulu, Hawaii. He was born in\n",
      "    responses: None\n"
     ]
    }
   ],
   "source": [
    "def my_generate(model, prompt, max_len=100, temperature = 0.8):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    print(model_inputs)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"\"\"My generate for prompt: {prompt}\n",
    "    responses: {my_generate(expert_lm, prompt)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- generation for plain model\n",
    "- comp between expert and amature lm\n",
    "- generation for contrastive decoding without restraint\n",
    "- generation for contrastive decoding with restraint\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
