{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation example with char-rnn\n",
    "\n",
    "First to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_text(url):\n",
    "    # Send a HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = 'https://homl.info/shakespeare'\n",
    "text_data = download_text(url)\n",
    "if text_data:\n",
    "    print(\"Data downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to download data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Vocab and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': 2, 's': 3, '-': 4, 'o': 5, 'q': 6, 'f': 7, 'z': 8, 'd': 9, 'k': 10, '&': 11, 'l': 12, 't': 13, 'c': 14, 'n': 15, ':': 16, 'm': 17, '.': 18, ' ': 19, 'h': 20, 'b': 21, 'j': 22, 'r': 23, 'u': 24, 'v': 25, ';': 26, 'p': 27, '\\n': 28, 'g': 29, 'a': 30, 'e': 31, '?': 32, 'w': 33, ',': 34, 'x': 35, '!': 36, '$': 37, \"'\": 38, 'i': 39, 'y': 40, '<UNK>': 1, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for char in text_data:\n",
    "    vocab.add(char.lower())\n",
    "\n",
    "vocab_mapping = {char:i+2 for i, char in enumerate(vocab)} # 0 for padding and 1 for unknown\n",
    "vocab_mapping['<UNK>'] = 1\n",
    "vocab_mapping['<PAD>'] = 0\n",
    "print(vocab_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1, 39, 23,  3, 13, 19,  1, 39, 13, 39,  8, 31, 15, 16, 28,  1, 31,\n",
      "           7,  5, 23, 31, 19, 33, 31, 19, 27, 23,  5, 14, 31, 31,  9, 19, 30,\n",
      "          15, 40, 19,  7, 24, 23, 13, 20, 31, 23, 34, 19, 20, 31, 30, 23, 19,\n",
      "          17, 31, 19,  3, 27, 31, 30, 10, 18, 28, 28,  1, 12, 12, 16, 28,  1,\n",
      "          27, 31, 30, 10, 34, 19,  3, 27, 31, 30, 10, 18, 28, 28,  1, 39, 23,\n",
      "           3, 13, 19,  1, 39, 13, 39,  8, 31, 15, 16, 28,  1,  5, 24],\n",
      "         [19, 30, 23, 31, 19, 30, 12, 12, 19, 23, 31,  3,  5, 12, 25, 31,  9,\n",
      "          19, 23, 30, 13, 20, 31, 23, 19, 13,  5, 19,  9, 39, 31, 19, 13, 20,\n",
      "          30, 15, 19, 13,  5, 19,  7, 30, 17, 39,  3, 20, 32, 28, 28,  1, 12,\n",
      "          12, 16, 28,  1, 31,  3,  5, 12, 25, 31,  9, 18, 19, 23, 31,  3,  5,\n",
      "          12, 25, 31,  9, 18, 28, 28,  1, 39, 23,  3, 13, 19,  1, 39, 13, 39,\n",
      "           8, 31, 15, 16, 28,  1, 39, 23,  3, 13, 34, 19, 40,  5, 24]]]), tensor([[[39, 23,  3, 13, 19,  1, 39, 13, 39,  8, 31, 15, 16, 28,  1, 31,  7,\n",
      "           5, 23, 31, 19, 33, 31, 19, 27, 23,  5, 14, 31, 31,  9, 19, 30, 15,\n",
      "          40, 19,  7, 24, 23, 13, 20, 31, 23, 34, 19, 20, 31, 30, 23, 19, 17,\n",
      "          31, 19,  3, 27, 31, 30, 10, 18, 28, 28,  1, 12, 12, 16, 28,  1, 27,\n",
      "          31, 30, 10, 34, 19,  3, 27, 31, 30, 10, 18, 28, 28,  1, 39, 23,  3,\n",
      "          13, 19,  1, 39, 13, 39,  8, 31, 15, 16, 28,  1,  5, 24, 19],\n",
      "         [30, 23, 31, 19, 30, 12, 12, 19, 23, 31,  3,  5, 12, 25, 31,  9, 19,\n",
      "          23, 30, 13, 20, 31, 23, 19, 13,  5, 19,  9, 39, 31, 19, 13, 20, 30,\n",
      "          15, 19, 13,  5, 19,  7, 30, 17, 39,  3, 20, 32, 28, 28,  1, 12, 12,\n",
      "          16, 28,  1, 31,  3,  5, 12, 25, 31,  9, 18, 19, 23, 31,  3,  5, 12,\n",
      "          25, 31,  9, 18, 28, 28,  1, 39, 23,  3, 13, 19,  1, 39, 13, 39,  8,\n",
      "          31, 15, 16, 28,  1, 39, 23,  3, 13, 34, 19, 40,  5, 24, 19]]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TextTransform:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Numericalize tokens\n",
    "        return [self.vocab.get(char, 1) for char in text]\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: str, transform=None, window_len = 100) -> None:\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.window_len = window_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - 2 * self.window_len # need next few tokens as label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx:idx+self.window_len]\n",
    "        label = self.data[idx+self.window_len:idx+2*self.window_len]\n",
    "        if self.transform:\n",
    "            return self.transform(text), self.transform(label)\n",
    "        else:\n",
    "            return text, label\n",
    "\n",
    "\n",
    "window_len = 100      \n",
    "def collate_self_supervision(batch):\n",
    "    texts, labels = zip(batch)\n",
    "    return torch.tensor(texts), torch.tensor(labels)\n",
    "\n",
    "\n",
    "\n",
    "transform = TextTransform(vocab_mapping)\n",
    "dataset = MyDataset(text_data, transform, window_len)\n",
    "data_loader = DataLoader(dataset, batch_size=2, collate_fn=collate_self_supervision)\n",
    "\n",
    "print(data_loader._get_iterator()._next_data())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self,n_tokens, window_len, emb_dim=16, GRU_dim=128):\n",
    "        super(CharRNN, self).__init__() \n",
    "        self.embedding = nn.Embedding(n_tokens, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, GRU_dim, batch_first=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(GRU_dim, n_tokens),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        emb = self.embedding(input) #[batch, window_len, emb_dim]\n",
    "        seq, _h_n = self.gru(emb) #[batch, window_len, GRU_dim]\n",
    "        return self.output(seq) # [batch, window_len, n_tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- prediction utils\n",
    "- training model\n",
    "- test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
