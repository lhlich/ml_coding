{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation example with char-rnn\n",
    "\n",
    "First to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_text(url):\n",
    "    # Send a HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "url = 'https://homl.info/shakespeare'\n",
    "text_data = download_text(url)\n",
    "if text_data:\n",
    "    print(\"Data downloaded successfully!\")\n",
    "else:\n",
    "    print(\"Failed to download data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Vocab and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o': 2, 'm': 3, 'v': 4, 'u': 5, 'h': 6, \"'\": 7, 't': 8, 'w': 9, 'b': 10, 'p': 11, 'c': 12, '&': 13, '-': 14, ' ': 15, '!': 16, 'j': 17, 'r': 18, 'q': 19, '$': 20, ';': 21, ':': 22, 'e': 23, '?': 24, 's': 25, 'n': 26, 'z': 27, 'i': 28, 'l': 29, 'g': 30, ',': 31, 'y': 32, '3': 33, 'x': 34, 'a': 35, 'd': 36, 'k': 37, '.': 38, 'f': 39, '<UNK>': 1, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for char in text_data:\n",
    "    if char != '\\n':\n",
    "        vocab.add(char.lower())\n",
    "\n",
    "vocab_mapping = {char:i+2 for i, char in enumerate(vocab)} # 0 for padding and 1 for unknown\n",
    "vocab_mapping['<UNK>'] = 1\n",
    "vocab_mapping['<PAD>'] = 0\n",
    "print(vocab_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of example data: [torch.Size([2, 100]), torch.Size([2, 100])]\n",
      "(tensor([[39, 28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 10, 23, 39,  2,\n",
      "         18, 23, 15,  9, 23, 15, 11, 18,  2, 12, 23, 23, 36, 15, 35, 26, 32, 15,\n",
      "         39,  5, 18,  8,  6, 23, 18, 31, 15,  6, 23, 35, 18, 15,  3, 23, 15, 25,\n",
      "         11, 23, 35, 37, 38, 35, 29, 29, 22, 25, 11, 23, 35, 37, 31, 15, 25, 11,\n",
      "         23, 35, 37, 38, 39, 28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22,\n",
      "         32,  2,  5, 15, 35, 18, 23, 15, 35, 29],\n",
      "        [28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 10, 23, 39,  2, 18,\n",
      "         23, 15,  9, 23, 15, 11, 18,  2, 12, 23, 23, 36, 15, 35, 26, 32, 15, 39,\n",
      "          5, 18,  8,  6, 23, 18, 31, 15,  6, 23, 35, 18, 15,  3, 23, 15, 25, 11,\n",
      "         23, 35, 37, 38, 35, 29, 29, 22, 25, 11, 23, 35, 37, 31, 15, 25, 11, 23,\n",
      "         35, 37, 38, 39, 28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 32,\n",
      "          2,  5, 15, 35, 18, 23, 15, 35, 29, 29]]), tensor([[28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 10, 23, 39,  2, 18,\n",
      "         23, 15,  9, 23, 15, 11, 18,  2, 12, 23, 23, 36, 15, 35, 26, 32, 15, 39,\n",
      "          5, 18,  8,  6, 23, 18, 31, 15,  6, 23, 35, 18, 15,  3, 23, 15, 25, 11,\n",
      "         23, 35, 37, 38, 35, 29, 29, 22, 25, 11, 23, 35, 37, 31, 15, 25, 11, 23,\n",
      "         35, 37, 38, 39, 28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 32,\n",
      "          2,  5, 15, 35, 18, 23, 15, 35, 29, 29],\n",
      "        [18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 10, 23, 39,  2, 18, 23,\n",
      "         15,  9, 23, 15, 11, 18,  2, 12, 23, 23, 36, 15, 35, 26, 32, 15, 39,  5,\n",
      "         18,  8,  6, 23, 18, 31, 15,  6, 23, 35, 18, 15,  3, 23, 15, 25, 11, 23,\n",
      "         35, 37, 38, 35, 29, 29, 22, 25, 11, 23, 35, 37, 31, 15, 25, 11, 23, 35,\n",
      "         37, 38, 39, 28, 18, 25,  8, 15, 12, 28,  8, 28, 27, 23, 26, 22, 32,  2,\n",
      "          5, 15, 35, 18, 23, 15, 35, 29, 29, 15]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TextTransform:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocab = vocabulary\n",
    "        self.n_tokens = len(vocabulary)\n",
    "        reverse_map = [None] * len(vocabulary)\n",
    "        for k, v in vocabulary.items():\n",
    "            reverse_map[v] = k\n",
    "        self.reverse_map = reverse_map\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Numericalize tokens\n",
    "        return [self.vocab.get(char.lower(), 1) for char in text]\n",
    "\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: str, transform=None, window_len = 100) -> None:\n",
    "        self.data = data.replace('\\n', '').lower()\n",
    "        self.transform = transform\n",
    "        self.window_len = window_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_len -1 # need next few tokens as label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx:idx+self.window_len]\n",
    "        label = self.data[idx+1:idx+self.window_len+1]\n",
    "        if self.transform:\n",
    "            return self.transform(text), self.transform(label)\n",
    "        else:\n",
    "            return text, label\n",
    "\n",
    "\n",
    "window_len = 100      \n",
    "def collate_self_supervision(batch):\n",
    "    texts, labels = zip(*batch)  # Unzip the tuples into separate lists\n",
    "    texts_tensor = torch.tensor(texts, dtype=torch.long)  # Ensure dtype is long for indices\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Same for labels\n",
    "    return texts_tensor, labels_tensor\n",
    "\n",
    "\n",
    "\n",
    "transform = TextTransform(vocab_mapping)\n",
    "dataset = MyDataset(text_data, transform, window_len)\n",
    "data_loader = DataLoader(dataset, batch_size=2, collate_fn=collate_self_supervision)\n",
    "\n",
    "example_data = data_loader._get_iterator()._next_data()\n",
    "print(f'shapes of example data: {[t.shape for t in example_data]}')\n",
    "print(example_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define your device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, n_tokens, emb_dim=16, GRU_dim=128):\n",
    "        super(CharRNN, self).__init__() \n",
    "        self.embedding = nn.Embedding(n_tokens, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, GRU_dim, batch_first=True)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(GRU_dim, 2*n_tokens),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*n_tokens, n_tokens),\n",
    "        )\n",
    "        self.GRU_dim = GRU_dim\n",
    "    \n",
    "    def forward(self, input):\n",
    "        emb = self.embedding(input)  # [batch, window_len, emb_dim]\n",
    "        seq, _h_n = self.gru(emb)    # [batch, window_len, GRU_dim]\n",
    "        return self.output(seq)       # [batch, window_len, n_tokens]\n",
    "    \n",
    "    def generate(self, input_tensor, length=5, temperature=0.8):\n",
    "        N = len(input_tensor)\n",
    "        h = torch.zeros(size=(1, N, self.GRU_dim)).to(device)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for _ in range(length):\n",
    "            if len(output) == 0:\n",
    "                input = input_tensor\n",
    "            else:\n",
    "                input = output[-1]\n",
    "            emb = self.embedding(input)  # [batch, window_len, emb_dim]\n",
    "            seq, h = self.gru(emb, h)    # [batch, window_len, GRU_dim]\n",
    "            logits_last = self.output(seq)[:,-1,:]      # [batch, 1, n_tokens]\n",
    "            prob = torch.softmax(logits_last.squeeze(1),dim=1) # [batch, n_tokens]\n",
    "            next_token = torch.multinomial(prob, num_samples=1) #[batch, 1]\n",
    "            output.append(next_token) # [batch, 1]\n",
    "\n",
    "        return output #[batch, length]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate(prompt: str, model, length=5, temperature=0.8) -> str:\n",
    "    input_tensor = torch.tensor(transform(prompt)).unsqueeze(0).to(device)\n",
    "    output_tokens = model.generate(input_tensor, length, temperature)  # Ensure this is on the right device\n",
    "    # print(output_tokens)\n",
    "    return ''.join([transform.reverse_map[idx] for idx in output_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Before training:\n",
      "    prompt: To be or not to be\n",
      "    next seq: b chc\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Move model to device\n",
    "model = CharRNN(transform.n_tokens).to(device)\n",
    "prompt = 'To be or not to be'\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Before training:\n",
    "    prompt: {prompt}\n",
    "    next seq: {generate(prompt, model, temperature=0.3)}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- epoch 1 ---\n",
      "0 batches processed(0.0%), ce loss -0.002575966063886881\n",
      "1000 batches processed(0.0018599564398201795%), ce loss -6388.337890625\n",
      "2000 batches processed(0.003719912879640359%), ce loss -25402.939453125\n",
      "3000 batches processed(0.005579869319460538%), ce loss -57269.44921875\n",
      "4000 batches processed(0.007439825759280718%), ce loss -100038.9375\n",
      "5000 batches processed(0.009299782199100897%), ce loss -157385.125\n",
      "6000 batches processed(0.011159738638921076%), ce loss -226237.875\n",
      "7000 batches processed(0.013019695078741256%), ce loss -308572.40625\n",
      "8000 batches processed(0.014879651518561436%), ce loss -403917.1875\n",
      "9000 batches processed(0.016739607958381614%), ce loss -506824.40625\n",
      "10000 batches processed(0.018599564398201793%), ce loss -636673.75\n",
      "11000 batches processed(0.020459520838021973%), ce loss -757458.9375\n",
      "12000 batches processed(0.022319477277842153%), ce loss -909946.0625\n",
      "13000 batches processed(0.024179433717662333%), ce loss -1075204.625\n",
      "14000 batches processed(0.026039390157482512%), ce loss -1235372.375\n",
      "15000 batches processed(0.027899346597302692%), ce loss -1427716.125\n",
      "16000 batches processed(0.02975930303712287%), ce loss -1626224.625\n",
      "17000 batches processed(0.03161925947694305%), ce loss -1816705.25\n",
      "18000 batches processed(0.03347921591676323%), ce loss -2040251.5\n",
      "19000 batches processed(0.03533917235658341%), ce loss -2264445.75\n",
      "20000 batches processed(0.03719912879640359%), ce loss -2518369.5\n",
      "21000 batches processed(0.03905908523622377%), ce loss -2762966.5\n",
      "22000 batches processed(0.040919041676043946%), ce loss -3060976.75\n",
      "23000 batches processed(0.04277899811586413%), ce loss -3335926.0\n",
      "24000 batches processed(0.044638954555684306%), ce loss -3569595.5\n",
      "25000 batches processed(0.04649891099550448%), ce loss -3923368.25\n",
      "26000 batches processed(0.048358867435324665%), ce loss -4224462.0\n",
      "27000 batches processed(0.05021882387514484%), ce loss -4599223.5\n",
      "28000 batches processed(0.052078780314965024%), ce loss -4952643.0\n",
      "29000 batches processed(0.0539387367547852%), ce loss -5290500.5\n",
      "30000 batches processed(0.055798693194605384%), ce loss -5663693.0\n",
      "31000 batches processed(0.05765864963442556%), ce loss -6069472.0\n",
      "32000 batches processed(0.05951860607424574%), ce loss -6479824.5\n",
      "33000 batches processed(0.06137856251406592%), ce loss -6906109.5\n",
      "34000 batches processed(0.0632385189538861%), ce loss -7361115.0\n",
      "35000 batches processed(0.06509847539370628%), ce loss -7722242.5\n",
      "36000 batches processed(0.06695843183352646%), ce loss -8233168.5\n",
      "37000 batches processed(0.06881838827334665%), ce loss -8615909.0\n",
      "38000 batches processed(0.07067834471316682%), ce loss -9134608.0\n",
      "39000 batches processed(0.072538301152987%), ce loss -9581599.0\n",
      "40000 batches processed(0.07439825759280717%), ce loss -10185541.0\n",
      "41000 batches processed(0.07625821403262735%), ce loss -10517570.0\n",
      "42000 batches processed(0.07811817047244754%), ce loss -11108252.0\n",
      "43000 batches processed(0.07997812691226772%), ce loss -11653027.0\n",
      "44000 batches processed(0.08183808335208789%), ce loss -12291635.0\n",
      "45000 batches processed(0.08369803979190807%), ce loss -12802307.0\n",
      "46000 batches processed(0.08555799623172826%), ce loss -13312824.0\n",
      "47000 batches processed(0.08741795267154844%), ce loss -14031652.0\n",
      "48000 batches processed(0.08927790911136861%), ce loss -14506322.0\n",
      "49000 batches processed(0.09113786555118879%), ce loss -15165755.0\n",
      "50000 batches processed(0.09299782199100896%), ce loss -15681370.0\n",
      "51000 batches processed(0.09485777843082915%), ce loss -16346235.0\n",
      "52000 batches processed(0.09671773487064933%), ce loss -17166168.0\n",
      "53000 batches processed(0.0985776913104695%), ce loss -17543952.0\n",
      "54000 batches processed(0.10043764775028968%), ce loss -18419238.0\n",
      "55000 batches processed(0.10229760419010987%), ce loss -19114426.0\n",
      "56000 batches processed(0.10415756062993005%), ce loss -19607548.0\n",
      "57000 batches processed(0.10601751706975023%), ce loss -20664872.0\n",
      "58000 batches processed(0.1078774735095704%), ce loss -21092400.0\n",
      "59000 batches processed(0.10973742994939059%), ce loss -21935206.0\n",
      "60000 batches processed(0.11159738638921077%), ce loss -22614722.0\n",
      "61000 batches processed(0.11345734282903094%), ce loss -23743238.0\n",
      "62000 batches processed(0.11531729926885112%), ce loss -24114286.0\n",
      "63000 batches processed(0.1171772557086713%), ce loss -24676022.0\n",
      "64000 batches processed(0.11903721214849149%), ce loss -25887074.0\n",
      "65000 batches processed(0.12089716858831166%), ce loss -26936852.0\n",
      "66000 batches processed(0.12275712502813184%), ce loss -27391596.0\n",
      "67000 batches processed(0.12461708146795202%), ce loss -28223286.0\n",
      "68000 batches processed(0.1264770379077722%), ce loss -28456488.0\n",
      "69000 batches processed(0.12833699434759238%), ce loss -30079488.0\n",
      "70000 batches processed(0.13019695078741256%), ce loss -30815386.0\n",
      "71000 batches processed(0.13205690722723273%), ce loss -31766584.0\n",
      "72000 batches processed(0.1339168636670529%), ce loss -32721408.0\n",
      "73000 batches processed(0.1357768201068731%), ce loss -33681540.0\n",
      "74000 batches processed(0.1376367765466933%), ce loss -34711052.0\n",
      "75000 batches processed(0.13949673298651347%), ce loss -35592240.0\n",
      "76000 batches processed(0.14135668942633364%), ce loss -36744592.0\n",
      "77000 batches processed(0.14321664586615382%), ce loss -37405672.0\n",
      "78000 batches processed(0.145076602305974%), ce loss -38329012.0\n",
      "79000 batches processed(0.14693655874579417%), ce loss -38979744.0\n",
      "80000 batches processed(0.14879651518561435%), ce loss -40352808.0\n",
      "81000 batches processed(0.15065647162543452%), ce loss -41573712.0\n",
      "82000 batches processed(0.1525164280652547%), ce loss -42425576.0\n",
      "83000 batches processed(0.1543763845050749%), ce loss -42966056.0\n",
      "84000 batches processed(0.15623634094489508%), ce loss -44806696.0\n",
      "85000 batches processed(0.15809629738471526%), ce loss -45446668.0\n",
      "86000 batches processed(0.15995625382453543%), ce loss -46920224.0\n",
      "87000 batches processed(0.1618162102643556%), ce loss -47735952.0\n",
      "88000 batches processed(0.16367616670417579%), ce loss -48593804.0\n",
      "89000 batches processed(0.16553612314399596%), ce loss -50088624.0\n",
      "90000 batches processed(0.16739607958381614%), ce loss -51181776.0\n",
      "91000 batches processed(0.1692560360236363%), ce loss -52068952.0\n",
      "92000 batches processed(0.17111599246345652%), ce loss -53965324.0\n",
      "93000 batches processed(0.1729759489032767%), ce loss -55023228.0\n",
      "94000 batches processed(0.17483590534309687%), ce loss -55596360.0\n",
      "95000 batches processed(0.17669586178291705%), ce loss -56219464.0\n",
      "96000 batches processed(0.17855581822273722%), ce loss -58339768.0\n",
      "97000 batches processed(0.1804157746625574%), ce loss -58824416.0\n",
      "98000 batches processed(0.18227573110237758%), ce loss -61149040.0\n",
      "99000 batches processed(0.18413568754219775%), ce loss -61105360.0\n",
      "100000 batches processed(0.18599564398201793%), ce loss -62705084.0\n",
      "101000 batches processed(0.18785560042183813%), ce loss -64415336.0\n",
      "102000 batches processed(0.1897155568616583%), ce loss -65295556.0\n",
      "103000 batches processed(0.19157551330147848%), ce loss -67326128.0\n",
      "104000 batches processed(0.19343546974129866%), ce loss -68157520.0\n",
      "105000 batches processed(0.19529542618111884%), ce loss -69981976.0\n",
      "106000 batches processed(0.197155382620939%), ce loss -70585224.0\n",
      "107000 batches processed(0.1990153390607592%), ce loss -71939296.0\n",
      "108000 batches processed(0.20087529550057937%), ce loss -73383456.0\n",
      "109000 batches processed(0.20273525194039957%), ce loss -75234024.0\n",
      "110000 batches processed(0.20459520838021975%), ce loss -76035352.0\n",
      "111000 batches processed(0.20645516482003992%), ce loss -77415272.0\n",
      "112000 batches processed(0.2083151212598601%), ce loss -79596512.0\n",
      "113000 batches processed(0.21017507769968027%), ce loss -80783344.0\n",
      "114000 batches processed(0.21203503413950045%), ce loss -82562840.0\n",
      "115000 batches processed(0.21389499057932063%), ce loss -83465832.0\n",
      "116000 batches processed(0.2157549470191408%), ce loss -84619592.0\n",
      "117000 batches processed(0.21761490345896098%), ce loss -86797088.0\n",
      "118000 batches processed(0.21947485989878118%), ce loss -88328840.0\n",
      "119000 batches processed(0.22133481633860136%), ce loss -90215488.0\n",
      "120000 batches processed(0.22319477277842154%), ce loss -91082832.0\n",
      "121000 batches processed(0.2250547292182417%), ce loss -91945552.0\n",
      "122000 batches processed(0.2269146856580619%), ce loss -93477936.0\n",
      "123000 batches processed(0.22877464209788206%), ce loss -95921656.0\n",
      "124000 batches processed(0.23063459853770224%), ce loss -97065960.0\n",
      "125000 batches processed(0.23249455497752242%), ce loss -99046440.0\n",
      "126000 batches processed(0.2343545114173426%), ce loss -100347232.0\n",
      "127000 batches processed(0.2362144678571628%), ce loss -102100128.0\n",
      "128000 batches processed(0.23807442429698297%), ce loss -103054072.0\n",
      "129000 batches processed(0.23993438073680315%), ce loss -104668296.0\n",
      "130000 batches processed(0.24179433717662333%), ce loss -106675344.0\n",
      "131000 batches processed(0.2436542936164435%), ce loss -109195288.0\n",
      "132000 batches processed(0.24551425005626368%), ce loss -109689704.0\n",
      "133000 batches processed(0.24737420649608385%), ce loss -112845312.0\n",
      "134000 batches processed(0.24923416293590403%), ce loss -113152024.0\n",
      "135000 batches processed(0.25109411937572423%), ce loss -113871584.0\n",
      "136000 batches processed(0.2529540758155444%), ce loss -117309432.0\n",
      "137000 batches processed(0.2548140322553646%), ce loss -117406392.0\n",
      "138000 batches processed(0.25667398869518476%), ce loss -120103096.0\n",
      "139000 batches processed(0.25853394513500494%), ce loss -122312168.0\n",
      "140000 batches processed(0.2603939015748251%), ce loss -123288928.0\n",
      "141000 batches processed(0.2622538580146453%), ce loss -125424936.0\n",
      "142000 batches processed(0.26411381445446547%), ce loss -127681504.0\n",
      "143000 batches processed(0.26597377089428564%), ce loss -128310744.0\n",
      "144000 batches processed(0.2678337273341058%), ce loss -130435368.0\n",
      "145000 batches processed(0.269693683773926%), ce loss -132632840.0\n",
      "146000 batches processed(0.2715536402137462%), ce loss -134951616.0\n",
      "147000 batches processed(0.27341359665356635%), ce loss -136000272.0\n",
      "148000 batches processed(0.2752735530933866%), ce loss -139053216.0\n",
      "149000 batches processed(0.27713350953320676%), ce loss -140554800.0\n",
      "150000 batches processed(0.27899346597302693%), ce loss -140822176.0\n",
      "151000 batches processed(0.2808534224128471%), ce loss -143494016.0\n",
      "152000 batches processed(0.2827133788526673%), ce loss -145929600.0\n",
      "153000 batches processed(0.28457333529248746%), ce loss -148629872.0\n",
      "154000 batches processed(0.28643329173230764%), ce loss -150600256.0\n",
      "155000 batches processed(0.2882932481721278%), ce loss -151703824.0\n",
      "156000 batches processed(0.290153204611948%), ce loss -153890368.0\n",
      "157000 batches processed(0.29201316105176817%), ce loss -154953408.0\n",
      "158000 batches processed(0.29387311749158834%), ce loss -158216688.0\n",
      "159000 batches processed(0.2957330739314085%), ce loss -161004992.0\n",
      "160000 batches processed(0.2975930303712287%), ce loss -160671040.0\n",
      "161000 batches processed(0.29945298681104887%), ce loss -161923248.0\n",
      "162000 batches processed(0.30131294325086905%), ce loss -166576944.0\n",
      "163000 batches processed(0.3031728996906892%), ce loss -166941424.0\n",
      "164000 batches processed(0.3050328561305094%), ce loss -169209648.0\n",
      "165000 batches processed(0.3068928125703296%), ce loss -173193488.0\n",
      "166000 batches processed(0.3087527690101498%), ce loss -174644064.0\n",
      "167000 batches processed(0.31061272544997%), ce loss -177110224.0\n",
      "168000 batches processed(0.31247268188979016%), ce loss -177384720.0\n",
      "169000 batches processed(0.31433263832961034%), ce loss -181423600.0\n",
      "170000 batches processed(0.3161925947694305%), ce loss -183468928.0\n",
      "171000 batches processed(0.3180525512092507%), ce loss -185440336.0\n",
      "172000 batches processed(0.31991250764907087%), ce loss -187205024.0\n",
      "173000 batches processed(0.32177246408889104%), ce loss -190388576.0\n",
      "174000 batches processed(0.3236324205287112%), ce loss -190080976.0\n",
      "175000 batches processed(0.3254923769685314%), ce loss -193343616.0\n",
      "176000 batches processed(0.32735233340835157%), ce loss -197420784.0\n",
      "177000 batches processed(0.32921228984817175%), ce loss -197668448.0\n",
      "178000 batches processed(0.3310722462879919%), ce loss -200162448.0\n",
      "179000 batches processed(0.3329322027278121%), ce loss -201377744.0\n",
      "180000 batches processed(0.3347921591676323%), ce loss -203052096.0\n",
      "181000 batches processed(0.33665211560745245%), ce loss -203758880.0\n",
      "182000 batches processed(0.3385120720472726%), ce loss -210589984.0\n",
      "183000 batches processed(0.34037202848709286%), ce loss -211422432.0\n",
      "184000 batches processed(0.34223198492691304%), ce loss -213351424.0\n",
      "185000 batches processed(0.3440919413667332%), ce loss -216788624.0\n",
      "186000 batches processed(0.3459518978065534%), ce loss -218234560.0\n",
      "187000 batches processed(0.34781185424637356%), ce loss -219909632.0\n",
      "188000 batches processed(0.34967181068619374%), ce loss -223712544.0\n",
      "189000 batches processed(0.3515317671260139%), ce loss -225382768.0\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = SGD(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "num_epoch = 1\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=2, collate_fn=collate_self_supervision, shuffle=True)\n",
    "for k in range(1, num_epoch+1):\n",
    "    print(f'--- epoch {k} ---')\n",
    "    try:\n",
    "        for i, data in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            texts, label = data     # label shape: [batch, seq_len]\n",
    "            pred = model(texts.to(device)) # batch, seq_len, n_tokens\n",
    "            loss = criterion(pred.transpose(1, 2), label.to(device))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(f'{i} batches processed({i/len(data_loader)}%), ce loss {loss}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}, batch idx {i}, epoch idx {k}\")\n",
    "\n",
    "    print(f\"After epoch {k}, ce loss is {loss}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CharRNN.generate() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBarack Obama was born in Honolulu, Hawaii. He was born in\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto be or not to be\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m    After training:\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m    prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;124m    next seq: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[94], line 49\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(prompt, model, length, temperature)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, model, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     48\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(transform(prompt))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 49\u001b[0m     output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure this is on the right device\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# print(output_tokens)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([transform\u001b[38;5;241m.\u001b[39mreverse_map[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m output_tokens])\n",
      "\u001b[1;31mTypeError\u001b[0m: CharRNN.generate() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "prompt = 'Barack Obama was born in Honolulu, Hawaii. He was born in'\n",
    "prompt = 'to be or not to be'\n",
    "print(\n",
    "    f\"\"\"\n",
    "    After training:\n",
    "    prompt: {prompt}\n",
    "    next seq: {generate(prompt, model, length=200)}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- sampling and temperature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
