{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The math\n",
    "$\\hat{y} = Xb$ where $\\hat{y}$ are predictions, $X$ are features and $b$ are trained model parameters\n",
    "\n",
    "For MSE we have closed form solution($y$ as labels):\n",
    "$\n",
    "b=(X^TX)^{-1}X^T y\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "n = 10 # 10 data\n",
    "k = 5 # num of feautres\n",
    "\n",
    "features = np.random.randn(n, k)\n",
    "labels = np.random.randn(n, 1)\n",
    "\n",
    "X = features\n",
    "y_hat = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y: NDArray, y_pred: NDArray) -> float:\n",
    "    return sum(((y-y_pred) ** 2)/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression with sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mse: [0.24471667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_reg_sklearn(X, y):\n",
    "    reg = LinearRegression(fit_intercept=False).fit(X, y)\n",
    "    print(f\"Mse: {MSE(y_hat, reg.predict(X))}\")\n",
    "\n",
    "linear_reg_sklearn(X, y_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression with closed form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.74354049]\n"
     ]
    }
   ],
   "source": [
    "def linear_reg_closed_form(X, y):\n",
    "    b_closed_form = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y_hat)\n",
    "    # print(b_closed_form)\n",
    "    print(f\"MSE: {MSE(y_hat, X.dot(b_closed_form))}\")\n",
    "\n",
    "linear_reg_closed_form(X, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression through gradient descent**\n",
    "\n",
    "The math:\n",
    "$\n",
    "L = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\sum_{j=1}^kx_{ij}b_j)^2\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial b_m} = -\\frac{2}{n}\\sum_{i=1}^n x_{im}(y_i-\\sum_{j=1}^kx_{ij}b_j)\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial b} = -\\frac{2}{n} X^T(y - Xb)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 10, loss [1.23534309]\n",
      "At epoch 20, loss [0.9463261]\n",
      "At epoch 30, loss [0.83242008]\n",
      "At epoch 40, loss [0.7826889]\n",
      "At epoch 50, loss [0.76080672]\n",
      "At epoch 60, loss [0.75115875]\n",
      "At epoch 70, loss [0.74690226]\n",
      "At epoch 80, loss [0.74502402]\n",
      "At epoch 90, loss [0.74419518]\n",
      "At epoch 100, loss [0.74382941]\n",
      "MSE: [0.74380671]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "max_epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "def linear_reg_gradient_descent(X, y_hat):\n",
    "    b_gd = np.random.randn(k, 1)\n",
    "    loss = []\n",
    "    for i in range(1, max_epochs+1):\n",
    "        pred = X.dot(b_gd)\n",
    "        loss.append(MSE(y_hat, pred))\n",
    "        gradient = -2/n * X.T.dot(y_hat - pred)\n",
    "        b_gd -= learning_rate*gradient\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'At epoch {i}, loss {loss[-1]}')\n",
    "\n",
    "    # print(b_gd)\n",
    "    print(f\"MSE: {MSE(y_hat, X.dot(b_gd))}\")\n",
    "\n",
    "linear_reg_gradient_descent(X, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's check how logistic regression works with classification problem**\n",
    "Firstly check normal linear regression and GD based linear regression\n",
    "$X : n*k, y:n*1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- linear regression by closed form ---\n",
      "MSE: [0.24471667]\n",
      "--- linear regression by gradient descent ---\n",
      "At epoch 10, loss [0.46517566]\n",
      "At epoch 20, loss [0.28263667]\n",
      "At epoch 30, loss [0.24675687]\n",
      "At epoch 40, loss [0.23608397]\n",
      "At epoch 50, loss [0.23207116]\n",
      "At epoch 60, loss [0.23039512]\n",
      "At epoch 70, loss [0.22966838]\n",
      "At epoch 80, loss [0.22934943]\n",
      "At epoch 90, loss [0.22920891]\n",
      "At epoch 100, loss [0.22914693]\n",
      "MSE: [0.22914309]\n",
      "--- linear regression by sklearn ---\n",
      "Mse: [0.69316996]\n"
     ]
    }
   ],
   "source": [
    "labels_binary = np.random.randint(low=0, high=2, size=(n,1))\n",
    "\n",
    "print(\"--- linear regression by closed form ---\")\n",
    "linear_reg_closed_form(X, labels_binary)\n",
    "print(\"--- linear regression by gradient descent ---\")\n",
    "linear_reg_gradient_descent(X, labels_binary)\n",
    "print(\"--- linear regression by sklearn ---\")\n",
    "linear_reg_sklearn(X, labels_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mse: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_reg_sklearn(X, y):\n",
    "    reg = LogisticRegression(fit_intercept=False).fit(X, y.reshape(-1))\n",
    "    print(f\"Mse: {MSE(y.reshape(-1), reg.predict(X))}\")\n",
    "\n",
    "logistic_reg_sklearn(X, labels_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression with gradient boost**\n",
    "The math: $\\hat{y} = \\sigma(Xb)$, let $y^p=Xb$. Then we have:\n",
    "\n",
    "$L = \\frac{1}{n}\\sum_{i=1}^n - [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1-\\hat{y}_i)]\n",
    "\\Rightarrow \\frac{\\partial L}{\\partial \\hat{y}_i} = \\frac{1}{n}(\\frac{1-y_i}{1-\\hat{y_i}}-\\frac{y_i}{\\hat{y}_i})\n",
    "$\n",
    "\n",
    "$\\frac{\\partial \\hat{y}_i}{\\partial y^p_i} = \\sigma(y^p_i)(1-\\sigma(y^p_i))$. For $i\\neq j$, $\\frac{\\partial \\hat{y}_i}{\\partial y^p_j} = 0$\n",
    "\n",
    "$\\frac{\\partial y^p_i}{\\partial b_j} = x_{ij}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial y^p_i} = \\frac{\\partial L}{\\partial \\hat{y}_i}  \\frac{\\partial \\hat{y}_i}{\\partial y^p_i}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_j} = \\sum_{i=1}^n \\frac{\\partial L}{\\partial y^p_i} x_{ij} \\Rightarrow \\frac{dL}{db} = X^T \\frac{dL}{dy^p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 10, CE loss: 0.7188416231639809, MSE: [0.22453573]\n",
      "At epoch 20, CE loss: 0.5973793703921088, MSE: [0.19748365]\n",
      "At epoch 30, CE loss: 0.5075192331701573, MSE: [0.17169645]\n",
      "At epoch 40, CE loss: 0.44453767404852096, MSE: [0.14797108]\n",
      "At epoch 50, CE loss: 0.4017175836897085, MSE: [0.12904094]\n",
      "At epoch 60, CE loss: 0.37200949666475475, MSE: [0.11529178]\n",
      "At epoch 70, CE loss: 0.35023992498163714, MSE: [0.10540733]\n",
      "At epoch 80, CE loss: 0.33331105251084914, MSE: [0.09804089]\n",
      "At epoch 90, CE loss: 0.319496133739193, MSE: [0.09229634]\n",
      "At epoch 100, CE loss: 0.3078253812628044, MSE: [0.08764142]\n"
     ]
    }
   ],
   "source": [
    "def logistic_reg_gradient_descent(X, y):\n",
    "    b_gd = np.random.randn(k, 1)\n",
    "    loss = []\n",
    "    for i in range(1, max_epochs+1):\n",
    "        yp = X.dot(b_gd)\n",
    "        y_hat =  1 / (1 + np.exp(-yp))\n",
    "        CE_array = -y*np.log(y_hat)-(1-y)*np.log(1-y_hat)\n",
    "        ce_loss = np.mean(CE_array)\n",
    "        loss.append(MSE(y_hat, y))\n",
    "        \n",
    "        dL_dYhat = ((1-y)/(1-y_hat) - y/y_hat) / n \n",
    "        dYhat_dYp = y_hat * (1-y_hat)\n",
    "        dL_dYp = dL_dYhat * dYhat_dYp\n",
    "\n",
    "        gradient = X.T.dot(dL_dYp)\n",
    "\n",
    "        b_gd -= learning_rate*gradient\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print(f'At epoch {i}, CE loss: {ce_loss}, MSE: {loss[-1]}')\n",
    "\n",
    "\n",
    "logistic_reg_gradient_descent(X, labels_binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
