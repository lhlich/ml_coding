{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhlich/ml_coding/blob/master/deep_learning/gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-uwFBuJHzW"
      },
      "source": [
        "## Gradient computation\n",
        "\n",
        "We'll show how to compute gradient in three approaches:\n",
        "- Analytical - vanilla: with the closed form of the original function, we can take the partial derivatives with respect to all variables, and implement the gradient function as another method\n",
        "  - We call it vanilla since we apply the definition directly\n",
        "- Numberical: gradient means how output changes with respect to a small shift of input, so we just simulate the process.\n",
        "- Analytical - back propagation: we treat the target function as a computation graph of a few known sub-functions with known analytical gradients. In this case the analytical gradient can be deduced from chain rule and implemented through back propagation algorithm\n",
        "\n",
        "\n",
        "#### Example function\n",
        "Target function: $y = f(\\mathbf{x}) = x_1^2 + x_2^2$, where $\\mathbf{x} = [x_1, x_2]^T$ is the variable vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwbwB7_0JHzX"
      },
      "source": [
        "#### Analytical gradient - vanilla\n",
        "\n",
        "Vanilla analytical gradient needs to deduce the closed form the gradient for any $\\mathbf{x}$. In this example obviously we have $\\frac{\\partial y}{\\partial x_i} = 2x_i$ and hence:\n",
        "\n",
        "$\\triangledown f = [2x_1, 2x_2]^T = 2\\mathbf{x} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9OK_J9XOJHzX",
        "outputId": "ddf04c54-1069-4bb2-c6b2-5cf4e110e257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analytical gradient: [6. 8.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def comp_grad_analytical(x):\n",
        "    return 2*x\n",
        "\n",
        "x = np.array([3.0, 4.0])\n",
        "print(f'analytical gradient: {comp_grad_analytical(x)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqygFMvaJHzY"
      },
      "source": [
        "#### Numerical gradient\n",
        "\n",
        "Numerical gradient is straightforward and generic:\n",
        "\n",
        "1. Set a small shift $\\epsilon$ for given variable, e.g. $x_1$. In this case denote $d\\mathbf{x}$ as $[\\epsilon, 0]^T$\n",
        "2. Compute $dy = f(\\mathbf{x} + d\\mathbf{x}) - f(\\mathbf{x})$ and the derivative w.r.t $x_1$ is roughly $\\frac{\\partial y}{\\partial x_1} \\approx dy/ \\epsilon $\n",
        "4. Repeat for $x_2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N-yd92jVJHzY",
        "outputId": "3afb0d91-b3ef-4ea3-dd5b-f6b8f1ec0079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numerical gradient: [6.1 8.1]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def comp_grad_numerical_generic(f, x, epsilon = 0.1):\n",
        "    grad = []\n",
        "    for i, x_i in enumerate(x):\n",
        "        dx = np.zeros_like(x)\n",
        "        dx[i] = epsilon\n",
        "        dy = f(x + dx) - f(x)\n",
        "        grad.append(dy/epsilon)\n",
        "\n",
        "    return np.array(grad).reshape(x.shape)\n",
        "\n",
        "def tar_func(x):\n",
        "    x_1, x_2 = x[0], x[1]\n",
        "    return x_1 * x_1 + x_2 * x_2\n",
        "\n",
        "x = np.array([3.0, 4.0])\n",
        "print(f'numerical gradient: {comp_grad_numerical_generic(tar_func, x)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfy6xYBvJHzZ"
      },
      "source": [
        "#### Analytical gradient - back propagation\n",
        "\n",
        "We will directly use pytorch to implement it. The essence here is that $f(\\mathbf{x}) = x_1^2 + x_2^2$ involves square operation and sum operation, which are the computation graph nodes with known gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "32qGUw_iJHzZ",
        "outputId": "3c614811-59c8-46d9-ebee-32b73c3ee39c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analytical gradient from back propagation: tensor([6., 8.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([3.0, 4.0], requires_grad=True)\n",
        "x_1, x_2 = x[0], x[1]\n",
        "y = x[0] * x[0] + x[1] * x[1] # y = torch.sum(x ** 2)\n",
        "y.backward()\n",
        "\n",
        "print(f'Analytical gradient from back propagation: {x.grad}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}